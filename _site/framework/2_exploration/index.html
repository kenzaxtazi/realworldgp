<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>  Framework - Gaussian Processes for Real-World Data</title>
<meta name="description" content="">



<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Gaussian Processes for Real-World Data">
<meta property="og:title" content="  Framework">
<meta property="og:url" content="http://localhost:4000/framework/2_exploration/">












  

  


<link rel="canonical" href="http://localhost:4000/framework/2_exploration/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Gaussian Processes for Real-World Data Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

<!-- <script>
    MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        svg: {
            fontCache: 'global'
        }
    };
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    type="text/javascript"></script> -->
<script>
    MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        svg: {
            fontCache: 'global'
        }
    };
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

\(
\newcommand{\bm}[1]{\boldsymbol{#1}}
\)
  </head>

  <body class="layout--archive">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Gaussian Processes for Real-World Data
          
        </a>
        <ul class="visible-links"></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<!-- <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  type="text/javascript"></script> -->

<div id="main" role="main">
  
  <div class="sidebar sticky">
  
  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle menu</label>
  <ul class="nav__items">
    
      <li>
        
          <a href="/"><span class="nav__sub-title">About</span></a>
        

        
      </li>
    
      <li>
        
          <a href="/introduction/"><span class="nav__sub-title">Introduction</span></a>
        

        
      </li>
    
      <li>
        
          <a href="/gp/"><span class="nav__sub-title">Gaussian Processes</span></a>
        

        
      </li>
    
      <li>
        
          <a href="/related_work/"><span class="nav__sub-title">Related Work</span></a>
        

        
      </li>
    
      <li>
        
          <a href="/framework/"><span class="nav__sub-title">Framework</span></a>
        

        
        <ul>
          
            <li><a href="/framework/1_problem_definition/">Step 1: Problem definition</a></li>
          
            <li><a href="/framework/2_exploration/" class="active">Step 2: Initial data exploration</a></li>
          
            <li><a href="/framework/3_domain_knowledge/">Step 3: Domain knowledge</a></li>
          
            <li><a href="/framework/4_set_definition/">Step 4: Set definition</a></li>
          
            <li><a href="/framework/5_scaling_structure/">Step 5: Scaling structure</a></li>
          
            <li><a href="/framework/6_transformation/">Step 6: Data transformations</a></li>
          
            <li><a href="/framework/7_kernel_design/">Step 7: Kernel design</a></li>
          
            <li><a href="/framework/8_model_iteration/">Step 8: Model iteration</a></li>
          
            <li><a href="/framework/9_scaling/">Step 9: Scaling</a></li>
          
            <li><a href="/framework/10_testing/">Step 10: Testing</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <a href="/software/"><span class="nav__sub-title">Software</span></a>
        

        
      </li>
    
      <li>
        
          <a href="/case_studies/"><span class="nav__sub-title">Case studies</span></a>
        

        
      </li>
    
      <li>
        
          <a href="/discussion/"><span class="nav__sub-title">Discussion</span></a>
        

        
      </li>
    
  </ul>
</nav>

    
  
  </div>



  <div class="archive">
    
    <h1 id="page-title" class="page__title">4. Framework</h1>
    
    <h2 id="step-2-initial-data-exploration">Step 2: Initial data exploration</h2>

<p>The next step is to perform an initial exploration of the data in order to understand whether it is suitable for GP regression. One should consider:</p>

<ul>
  <li>Number of data points $N$. For $N &gt; 10^4-10^5$, exact GP computation becomes prohibitively expensive (<a href="https://arxiv.org/abs/1502.02843">Deisenroth and Ng., 2015</a>; <a href="https://arxiv.org/abs/1903.08114">Wang et al., 2019</a>). $N &lt; 100$ may be too small especially in the case of a complex kernel with many hyperparameters which can lead to overfitting. Smaller $N$ can be mitigated using Markov Chain Monte Carlo (MCMC) estimation (<a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwj6mcz6_5OCAxVmUUEAHcHnBToQFnoECBYQAQ&amp;url=https%3A%2F%2Fwww.tandfonline.com%2Fdoi%2Fabs%2F10.1080%2F10705511.2016.1186549&amp;usg=AOvVaw0ZidbH7fi2dLBxWTPctuIB&amp;opi=89978449">McNeish et al., 2016</a>).</li>
  <li>Number of input dimensions $D$. GPs are not immune to the curse of dimensionality. For $D &gt; 10$, it is hard for the modeller to form a clear image of the problem. It can therefore also be difficult to design an appropriate kernel. The number of parameters used to define the kernel will also increase, meaning it will be easier to overfit. Furthermore, if $D$ is very large $D&gt;100$, constructing the covariance matrix, which scales as $\mathcal{O}(N^2D)$, can become a computational bottleneck.</li>
  <li>Output requirements. Are probability distributions needed for this task? If the modeller simply requires the mean output an alternative method may be more appropriate.</li>
</ul>

<p>Further considerations:</p>

<ul>
  <li>We propose a range of values for the upper limit of $N$. This is because the limit will depend on how the model is used. Higher $N$ can be used for one-off modelling rather than learning hyperparameters through repeated likelihood evaluation.</li>
  <li>Above these limits, it is worth considering the use of a GP as a wrapper for a deep learning model (<a href="https://hess.copernicus.org/articles/26/5163/2022/hess-26-5163-2022.html">Sun et al., 2022</a>). In this case, a deep learning model can be trained on a large subset of the data. The inputs to the GP can be the output from the deep learning model or their residuals. The GP could also be used to refine the predictions for specific locations in the input feature space using held-out data. This will then yield uncertainties which can be used for uncertainty quantification, active learning, etc.. $^{\dagger}$</li>
  <li>It may be acceptable to work in the top end of the dimension range if only a few dimensions are doing most of the predictive work. Furthermore, it is also possible to select or generate a set of lower dimensional features to feed into the model using decision trees such as Random Forests or dimensionality reduction methods such as Principal Component Analysi (<a href="https://arxiv.org/abs/2111.05040">Binois and Wycoff, 2022</a>).</li>
  <li>For large datasets (see Step 5), two approaches are possible. In the first case, the data can be divided into chunks, independent GPs or `GP expertsâ€™ applied are then to each part and predictions are made using a (robust) Bayesian Committee Machine (<a href="https://www.dbs.ifi.lmu.de/~tresp/papers/bcm6.pdf">Tresp, 2000</a>; <a href="https://arxiv.org/abs/1502.02843">Deisenroth and Ng, 2015</a>}. In the second case and conditional on specific structure, scaling GP methods can be also applied.</li>
</ul>

<p>$^{\dagger}$ If the models are not trained jointly, the procedure will result in overfitting and the residuals will go to zero with the GP collapsing to 0 uncertainty. If the GP is fit on the training data (and not the just the held-out data) using a neural network will be more likely to result in overfitting since the model might fit the data perfectly even before applying the GP.</p>

  </div>
</div>
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 Gaussian Processes for Real-World Data. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>










  </body>
</html>
